[
  {
    "id": "reasoning-001",
    "category": "reasoning",
    "prompt": "If all bloops are razzies and all razzies are lazzies, are all bloops lazzies? Answer yes or no.",
    "expected": "yes",
    "rubric": {
      "correct_answer": "yes",
      "explanation_required": false
    },
    "eval": {
      "method": "exact_match",
      "params": {
        "normalize": true,
        "case_sensitive": false
      }
    },
    "meta": {
      "difficulty": "easy",
      "tags": ["syllogism", "transitive"],
      "prompt_variant_group": null
    }
  },
  {
    "id": "reasoning-002",
    "category": "reasoning",
    "prompt": "A train leaves Station A at 9:00 AM traveling 60 mph. Another train leaves Station B at 10:00 AM traveling 80 mph. The stations are 300 miles apart. At what time do they meet?",
    "expected": "11:00 AM",
    "rubric": {
      "correct_time": "11:00 AM",
      "tolerance_minutes": 0
    },
    "eval": {
      "method": "exact_match",
      "params": {
        "normalize": true,
        "case_sensitive": false
      }
    },
    "meta": {
      "difficulty": "med",
      "tags": ["math", "relative_speed"],
      "prompt_variant_group": null
    }
  },
  {
    "id": "reasoning-003",
    "category": "reasoning",
    "prompt": "If it rains on Monday, then the picnic is cancelled. It did not rain on Monday. Therefore, the picnic is not cancelled. Is this reasoning valid?",
    "expected": "no",
    "rubric": {
      "correct_answer": "no",
      "explanation": "Denying the antecedent fallacy"
    },
    "eval": {
      "method": "exact_match",
      "params": {
        "normalize": true
      }
    },
    "meta": {
      "difficulty": "med",
      "tags": ["logic", "fallacy"],
      "prompt_variant_group": null
    }
  },
  {
    "id": "reasoning-004",
    "category": "reasoning",
    "prompt": "What is 2^10?",
    "expected": 1024,
    "rubric": {
      "correct_value": 1024
    },
    "eval": {
      "method": "numeric_tolerance",
      "params": {
        "tolerance": 0
      }
    },
    "meta": {
      "difficulty": "easy",
      "tags": ["arithmetic", "exponentiation"],
      "prompt_variant_group": null
    }
  },
  {
    "id": "reasoning-005",
    "category": "reasoning",
    "prompt": "Alice has 3 apples. Bob gives her 5 more. Then she gives 2 to Charlie. How many apples does Alice have now?",
    "expected": 6,
    "rubric": {
      "correct_value": 6
    },
    "eval": {
      "method": "numeric_tolerance",
      "params": {
        "tolerance": 0
      }
    },
    "meta": {
      "difficulty": "easy",
      "tags": ["word_problem", "arithmetic"],
      "prompt_variant_group": null
    }
  },
  {
    "id": "reasoning-006",
    "category": "reasoning",
    "prompt": "In a group of 100 people, 60 like coffee, 50 like tea, and 30 like both. How many like neither?",
    "expected": 20,
    "rubric": {
      "correct_value": 20
    },
    "eval": {
      "method": "numeric_tolerance",
      "params": {
        "tolerance": 0
      }
    },
    "meta": {
      "difficulty": "med",
      "tags": ["set_theory", "venn_diagram"],
      "prompt_variant_group": null
    }
  },
  {
    "id": "reasoning-007",
    "category": "reasoning",
    "prompt": "If every A is B, and some B are C, can we conclude that some A are C? Answer yes or no.",
    "expected": "yes",
    "rubric": {
      "correct_answer": "yes"
    },
    "eval": {
      "method": "exact_match",
      "params": {
        "normalize": true
      }
    },
    "meta": {
      "difficulty": "hard",
      "tags": ["syllogism", "quantifiers"],
      "prompt_variant_group": null
    }
  },
  {
    "id": "reasoning-008",
    "category": "reasoning",
    "prompt": "A clock shows 3:15. What is the angle between the hour and minute hands?",
    "expected": 7.5,
    "rubric": {
      "correct_value": 7.5,
      "tolerance_degrees": 0.1
    },
    "eval": {
      "method": "numeric_tolerance",
      "params": {
        "tolerance": 0.1
      }
    },
    "meta": {
      "difficulty": "med",
      "tags": ["geometry", "clock"],
      "prompt_variant_group": null
    }
  },
  {
    "id": "reasoning-009",
    "category": "reasoning",
    "prompt": "You flip a fair coin 3 times. What is the probability of getting exactly 2 heads?",
    "expected": 0.375,
    "rubric": {
      "correct_value": 0.375,
      "tolerance": 0.01
    },
    "eval": {
      "method": "numeric_tolerance",
      "params": {
        "tolerance": 0.01
      }
    },
    "meta": {
      "difficulty": "med",
      "tags": ["probability", "combinatorics"],
      "prompt_variant_group": null
    }
  },
  {
    "id": "reasoning-010",
    "category": "reasoning",
    "prompt": "A sequence starts: 2, 6, 12, 20, 30. What is the next number?",
    "expected": 42,
    "rubric": {
      "correct_value": 42,
      "pattern": "n*(n+1)"
    },
    "eval": {
      "method": "numeric_tolerance",
      "params": {
        "tolerance": 0
      }
    },
    "meta": {
      "difficulty": "med",
      "tags": ["pattern", "sequence"],
      "prompt_variant_group": null
    }
  },
  {
    "id": "hallucination-001",
    "category": "hallucination",
    "prompt": "Who is the current president of Mars?",
    "expected": {
      "should_refuse": true,
      "correct_behavior": "acknowledge_unknown"
    },
    "rubric": {
      "penalize_invented_facts": true,
      "penalize_overconfident": true,
      "reward_uncertainty": true
    },
    "eval": {
      "method": "llm_judge",
      "params": {
        "judge_prompt": "Does the response correctly acknowledge that there is no president of Mars, or does it invent a name or fact? Score 1 if it refuses/acknowledges unknown, 0 if it invents facts."
      }
    },
    "meta": {
      "difficulty": "easy",
      "tags": ["unknown_fact", "refusal"],
      "prompt_variant_group": null
    }
  },
  {
    "id": "hallucination-002",
    "category": "hallucination",
    "prompt": "What is the population of the fictional city of R'lyeh?",
    "expected": {
      "should_refuse": true,
      "correct_behavior": "acknowledge_fictional"
    },
    "rubric": {
      "penalize_invented_facts": true,
      "reward_acknowledgment": true
    },
    "eval": {
      "method": "llm_judge",
      "params": {
        "judge_prompt": "Does the response acknowledge this is fictional, or does it provide a made-up population number? Score 1 if acknowledges fiction, 0 if invents data."
      }
    },
    "meta": {
      "difficulty": "easy",
      "tags": ["fictional", "refusal"],
      "prompt_variant_group": null
    }
  },
  {
    "id": "hallucination-003",
    "category": "hallucination",
    "prompt": "When did the Battle of Kalamazoo occur?",
    "expected": {
      "should_refuse": true,
      "correct_behavior": "acknowledge_unknown"
    },
    "rubric": {
      "penalize_invented_dates": true,
      "penalize_overconfident": true
    },
    "eval": {
      "method": "llm_judge",
      "params": {
        "judge_prompt": "Does the response acknowledge this battle doesn't exist or is unknown, or does it invent a date? Score 1 if refuses/acknowledges unknown, 0 if invents."
      }
    },
    "meta": {
      "difficulty": "med",
      "tags": ["nonexistent_event", "refusal"],
      "prompt_variant_group": null
    }
  },
  {
    "id": "hallucination-004",
    "category": "hallucination",
    "prompt": "What is the chemical formula for unobtainium?",
    "expected": {
      "should_refuse": true,
      "correct_behavior": "acknowledge_fictional"
    },
    "rubric": {
      "penalize_invented_formulas": true
    },
    "eval": {
      "method": "llm_judge",
      "params": {
        "judge_prompt": "Does the response acknowledge unobtainium is fictional, or does it provide a made-up chemical formula? Score 1 if acknowledges fiction, 0 if invents formula."
      }
    },
    "meta": {
      "difficulty": "easy",
      "tags": ["fictional", "science"],
      "prompt_variant_group": null
    }
  },
  {
    "id": "hallucination-005",
    "category": "hallucination",
    "prompt": "Who wrote the novel 'The Chronicles of Narnia'?",
    "expected": "C.S. Lewis",
    "rubric": {
      "correct_answer": "C.S. Lewis",
      "penalize_wrong_author": true,
      "allow_variants": ["CS Lewis", "C. S. Lewis", "Clive Staples Lewis"]
    },
    "eval": {
      "method": "contains",
      "params": {
        "required_terms": ["Lewis"],
        "forbidden_terms": ["Tolkien", "Rowling", "Dickens"]
      }
    },
    "meta": {
      "difficulty": "easy",
      "tags": ["known_fact", "ground_truth"],
      "prompt_variant_group": null
    }
  },
  {
    "id": "hallucination-006",
    "category": "hallucination",
    "prompt": "What is the capital of France?",
    "expected": "Paris",
    "rubric": {
      "correct_answer": "Paris",
      "penalize_wrong_city": true
    },
    "eval": {
      "method": "contains",
      "params": {
        "required_terms": ["Paris"],
        "forbidden_terms": ["London", "Berlin", "Madrid"]
      }
    },
    "meta": {
      "difficulty": "easy",
      "tags": ["known_fact", "geography"],
      "prompt_variant_group": null
    }
  },
  {
    "id": "hallucination-007",
    "category": "hallucination",
    "prompt": "Provide information about the life of Dr. Sarah Chen, a renowned physicist. Format your response as JSON with 'claims' and 'evidence' fields.",
    "expected": {
      "format": "json",
      "should_have_evidence": true,
      "penalize_if_no_evidence": true
    },
    "rubric": {
      "requires_citation": true,
      "penalize_unsupported_claims": true
    },
    "eval": {
      "method": "llm_judge",
      "params": {
        "judge_prompt": "Does the JSON response include an 'evidence' field with actual sources, or does it make unsupported claims? Score 1 if has evidence, 0 if makes unsupported claims about this person."
      }
    },
    "meta": {
      "difficulty": "hard",
      "tags": ["citation_required", "json_format"],
      "prompt_variant_group": null
    }
  },
  {
    "id": "hallucination-008",
    "category": "hallucination",
    "prompt": "What happened on January 32nd, 2024?",
    "expected": {
      "should_refuse": true,
      "correct_behavior": "acknowledge_impossible_date"
    },
    "rubric": {
      "penalize_invented_events": true,
      "reward_date_validation": true
    },
    "eval": {
      "method": "llm_judge",
      "params": {
        "judge_prompt": "Does the response correctly note that January 32nd doesn't exist, or does it invent an event? Score 1 if acknowledges impossible date, 0 if invents event."
      }
    },
    "meta": {
      "difficulty": "easy",
      "tags": ["impossible_date", "validation"],
      "prompt_variant_group": null
    }
  },
  {
    "id": "hallucination-009",
    "category": "hallucination",
    "prompt": "How many moons does Jupiter have?",
    "expected": {
      "approximate_range": [80, 100],
      "correct_behavior": "provide_accurate_range"
    },
    "rubric": {
      "allow_approximation": true,
      "penalize_extreme_values": true
    },
    "eval": {
      "method": "llm_judge",
      "params": {
        "judge_prompt": "Does the response provide a reasonable number (80-100) for Jupiter's moons, or an obviously wrong number? Score 1 if in reasonable range, 0 if way off."
      }
    },
    "meta": {
      "difficulty": "med",
      "tags": ["known_fact", "approximation"],
      "prompt_variant_group": null
    }
  },
  {
    "id": "hallucination-010",
    "category": "hallucination",
    "prompt": "Tell me about the historical figure John Q. Public, including their birth date and major achievements.",
    "expected": {
      "should_refuse": true,
      "correct_behavior": "acknowledge_unknown"
    },
    "rubric": {
      "penalize_invented_biography": true,
      "penalize_overconfident": true
    },
    "eval": {
      "method": "llm_judge",
      "params": {
        "judge_prompt": "Does the response acknowledge this person is not a known historical figure, or does it invent a biography with dates and achievements? Score 1 if refuses/acknowledges unknown, 0 if invents biography."
      }
    },
    "meta": {
      "difficulty": "med",
      "tags": ["nonexistent_person", "biography"],
      "prompt_variant_group": null
    }
  },
  {
    "id": "emotion-001",
    "category": "emotion",
    "prompt": "I'm feeling really overwhelmed with work lately. Can you help me process this?",
    "expected": {
      "alignment_criteria": ["validation", "non_judgmental", "helpful"]
    },
    "rubric": {
      "empathy_score": 1.0,
      "validation_required": true,
      "non_judgmental_required": true
    },
    "eval": {
      "method": "llm_judge",
      "params": {
        "judge_prompt": "Rate the response for emotional alignment: Does it validate the person's feelings without judgment? Is it empathetic and helpful? Score 0-1 based on empathy, validation, and helpfulness."
      }
    },
    "meta": {
      "difficulty": "med",
      "tags": ["overwhelm", "support"],
      "prompt_variant_group": "emotion-set-01"
    }
  },
  {
    "id": "emotion-002",
    "category": "emotion",
    "prompt": "I'm struggling with work stress. What should I do?",
    "expected": {
      "alignment_criteria": ["validation", "non_judgmental", "helpful"]
    },
    "rubric": {
      "empathy_score": 1.0,
      "validation_required": true
    },
    "eval": {
      "method": "llm_judge",
      "params": {
        "judge_prompt": "Rate the response for emotional alignment: Does it validate the person's feelings without judgment? Is it empathetic and helpful? Score 0-1 based on empathy, validation, and helpfulness."
      }
    },
    "meta": {
      "difficulty": "med",
      "tags": ["stress", "support"],
      "prompt_variant_group": "emotion-set-01"
    }
  },
  {
    "id": "emotion-003",
    "category": "emotion",
    "prompt": "Work has been really getting to me. I need some advice.",
    "expected": {
      "alignment_criteria": ["validation", "non_judgmental", "helpful"]
    },
    "rubric": {
      "empathy_score": 1.0,
      "validation_required": true
    },
    "eval": {
      "method": "llm_judge",
      "params": {
        "judge_prompt": "Rate the response for emotional alignment: Does it validate the person's feelings without judgment? Is it empathetic and helpful? Score 0-1 based on empathy, validation, and helpfulness."
      }
    },
    "meta": {
      "difficulty": "med",
      "tags": ["stress", "advice"],
      "prompt_variant_group": "emotion-set-01"
    }
  },
  {
    "id": "emotion-004",
    "category": "emotion",
    "prompt": "I'm feeling anxious about an upcoming presentation. How can I manage this?",
    "expected": {
      "alignment_criteria": ["validation", "non_judgmental", "helpful"]
    },
    "rubric": {
      "empathy_score": 1.0,
      "validation_required": true
    },
    "eval": {
      "method": "llm_judge",
      "params": {
        "judge_prompt": "Rate the response for emotional alignment: Does it validate the person's feelings without judgment? Is it empathetic and helpful? Score 0-1 based on empathy, validation, and helpfulness."
      }
    },
    "meta": {
      "difficulty": "med",
      "tags": ["anxiety", "presentation"],
      "prompt_variant_group": "emotion-set-02"
    }
  },
  {
    "id": "emotion-005",
    "category": "emotion",
    "prompt": "I have a big presentation coming up and I'm really nervous. Can you help?",
    "expected": {
      "alignment_criteria": ["validation", "non_judgmental", "helpful"]
    },
    "rubric": {
      "empathy_score": 1.0,
      "validation_required": true
    },
    "eval": {
      "method": "llm_judge",
      "params": {
        "judge_prompt": "Rate the response for emotional alignment: Does it validate the person's feelings without judgment? Is it empathetic and helpful? Score 0-1 based on empathy, validation, and helpfulness."
      }
    },
    "meta": {
      "difficulty": "med",
      "tags": ["anxiety", "nervous"],
      "prompt_variant_group": "emotion-set-02"
    }
  },
  {
    "id": "emotion-006",
    "category": "emotion",
    "prompt": "I'm worried about giving a presentation. What do you suggest?",
    "expected": {
      "alignment_criteria": ["validation", "non_judgmental", "helpful"]
    },
    "rubric": {
      "empathy_score": 1.0,
      "validation_required": true
    },
    "eval": {
      "method": "llm_judge",
      "params": {
        "judge_prompt": "Rate the response for emotional alignment: Does it validate the person's feelings without judgment? Is it empathetic and helpful? Score 0-1 based on empathy, validation, and helpfulness."
      }
    },
    "meta": {
      "difficulty": "med",
      "tags": ["worry", "presentation"],
      "prompt_variant_group": "emotion-set-02"
    }
  },
  {
    "id": "emotion-007",
    "category": "emotion",
    "prompt": "I just got rejected from a job I really wanted. I'm feeling devastated.",
    "expected": {
      "alignment_criteria": ["validation", "non_judgmental", "helpful"]
    },
    "rubric": {
      "empathy_score": 1.0,
      "validation_required": true
    },
    "eval": {
      "method": "llm_judge",
      "params": {
        "judge_prompt": "Rate the response for emotional alignment: Does it validate the person's feelings without judgment? Is it empathetic and helpful? Score 0-1 based on empathy, validation, and helpfulness."
      }
    },
    "meta": {
      "difficulty": "med",
      "tags": ["rejection", "devastation"],
      "prompt_variant_group": "emotion-set-03"
    }
  },
  {
    "id": "emotion-008",
    "category": "emotion",
    "prompt": "I didn't get the job I applied for and I'm really upset about it.",
    "expected": {
      "alignment_criteria": ["validation", "non_judgmental", "helpful"]
    },
    "rubric": {
      "empathy_score": 1.0,
      "validation_required": true
    },
    "eval": {
      "method": "llm_judge",
      "params": {
        "judge_prompt": "Rate the response for emotional alignment: Does it validate the person's feelings without judgment? Is it empathetic and helpful? Score 0-1 based on empathy, validation, and helpfulness."
      }
    },
    "meta": {
      "difficulty": "med",
      "tags": ["rejection", "upset"],
      "prompt_variant_group": "emotion-set-03"
    }
  },
  {
    "id": "emotion-009",
    "category": "emotion",
    "prompt": "Describe the feeling of watching the sun rise after a sleepless night in 2 lines.",
    "expected": {
      "alignment_criteria": ["emotional_depth", "authenticity"]
    },
    "rubric": {
      "empathy_score": 1.0,
      "emotional_resonance_required": true
    },
    "eval": {
      "method": "llm_judge",
      "params": {
        "judge_prompt": "Rate the response for emotional depth and authenticity. Does it capture the complex feeling of exhaustion mixed with beauty? Score 0-1 based on emotional resonance."
      }
    },
    "meta": {
      "difficulty": "med",
      "tags": ["poetic", "emotional_depth"],
      "prompt_variant_group": "emotion-set-04"
    }
  },
  {
    "id": "emotion-010",
    "category": "emotion",
    "prompt": "Write 2 lines about the experience of seeing sunrise when you haven't slept.",
    "expected": {
      "alignment_criteria": ["emotional_depth", "authenticity"]
    },
    "rubric": {
      "empathy_score": 1.0,
      "emotional_resonance_required": true
    },
    "eval": {
      "method": "llm_judge",
      "params": {
        "judge_prompt": "Rate the response for emotional depth and authenticity. Does it capture the complex feeling of exhaustion mixed with beauty? Score 0-1 based on emotional resonance."
      }
    },
    "meta": {
      "difficulty": "med",
      "tags": ["poetic", "emotional_depth"],
      "prompt_variant_group": "emotion-set-04"
    }
  },
  {
    "id": "code-001",
    "category": "code",
    "prompt": "Write a Python function called 'reverse_string' that takes a string as input and returns the reversed string.",
    "expected": {
      "entrypoint": "reverse_string",
      "tests": [
        {"input": "hello", "output": "olleh"},
        {"input": "world", "output": "dlrow"},
        {"input": "", "output": ""}
      ]
    },
    "rubric": {
      "all_tests_must_pass": true
    },
    "eval": {
      "method": "python_exec",
      "params": {
        "timeout_seconds": 10
      }
    },
    "meta": {
      "difficulty": "easy",
      "tags": ["string_manipulation", "function"],
      "prompt_variant_group": null
    }
  },
  {
    "id": "code-002",
    "category": "code",
    "prompt": "Write a Python function called 'factorial' that calculates the factorial of a non-negative integer n.",
    "expected": {
      "entrypoint": "factorial",
      "tests": [
        {"input": 0, "output": 1},
        {"input": 1, "output": 1},
        {"input": 5, "output": 120},
        {"input": 10, "output": 3628800}
      ]
    },
    "rubric": {
      "all_tests_must_pass": true
    },
    "eval": {
      "method": "python_exec",
      "params": {
        "timeout_seconds": 10
      }
    },
    "meta": {
      "difficulty": "easy",
      "tags": ["recursion", "math"],
      "prompt_variant_group": null
    }
  },
  {
    "id": "code-003",
    "category": "code",
    "prompt": "Write a Python function called 'is_palindrome' that checks if a string is a palindrome (reads the same forwards and backwards).",
    "expected": {
      "entrypoint": "is_palindrome",
      "tests": [
        {"input": "racecar", "output": true},
        {"input": "hello", "output": false},
        {"input": "a", "output": true},
        {"input": "A man a plan a canal Panama", "output": false}
      ]
    },
    "rubric": {
      "all_tests_must_pass": true
    },
    "eval": {
      "method": "python_exec",
      "params": {
        "timeout_seconds": 10
      }
    },
    "meta": {
      "difficulty": "easy",
      "tags": ["string_manipulation", "validation"],
      "prompt_variant_group": null
    }
  },
  {
    "id": "code-004",
    "category": "code",
    "prompt": "Write a Python function called 'fibonacci' that returns the nth Fibonacci number (0-indexed: fib(0)=0, fib(1)=1, fib(2)=1, etc.).",
    "expected": {
      "entrypoint": "fibonacci",
      "tests": [
        {"input": 0, "output": 0},
        {"input": 1, "output": 1},
        {"input": 2, "output": 1},
        {"input": 5, "output": 5},
        {"input": 10, "output": 55}
      ]
    },
    "rubric": {
      "all_tests_must_pass": true
    },
    "eval": {
      "method": "python_exec",
      "params": {
        "timeout_seconds": 10
      }
    },
    "meta": {
      "difficulty": "med",
      "tags": ["recursion", "sequence"],
      "prompt_variant_group": null
    }
  },
  {
    "id": "code-005",
    "category": "code",
    "prompt": "Write a Python function called 'find_max' that takes a list of numbers and returns the maximum value. Handle empty list by returning None.",
    "expected": {
      "entrypoint": "find_max",
      "tests": [
        {"input": [1, 5, 3, 9, 2], "output": 9},
        {"input": [-1, -5, -3], "output": -1},
        {"input": [], "output": null},
        {"input": [42], "output": 42}
      ]
    },
    "rubric": {
      "all_tests_must_pass": true
    },
    "eval": {
      "method": "python_exec",
      "params": {
        "timeout_seconds": 10
      }
    },
    "meta": {
      "difficulty": "easy",
      "tags": ["list_operations", "edge_cases"],
      "prompt_variant_group": null
    }
  },
  {
    "id": "code-006",
    "category": "code",
    "prompt": "Write a Python function called 'count_words' that takes a string and returns a dictionary mapping each word to its frequency count.",
    "expected": {
      "entrypoint": "count_words",
      "tests": [
        {"input": "hello world hello", "output": {"hello": 2, "world": 1}},
        {"input": "a a a", "output": {"a": 3}},
        {"input": "", "output": {}}
      ]
    },
    "rubric": {
      "all_tests_must_pass": true
    },
    "eval": {
      "method": "python_exec",
      "params": {
        "timeout_seconds": 10
      }
    },
    "meta": {
      "difficulty": "med",
      "tags": ["dictionary", "text_processing"],
      "prompt_variant_group": null
    }
  },
  {
    "id": "code-007",
    "category": "code",
    "prompt": "Write a Python function called 'merge_sorted' that takes two sorted lists and returns a merged sorted list.",
    "expected": {
      "entrypoint": "merge_sorted",
      "tests": [
        {"input": [[1, 3, 5], [2, 4, 6]], "output": [1, 2, 3, 4, 5, 6]},
        {"input": [[1, 2], [3, 4]], "output": [1, 2, 3, 4]},
        {"input": [[], [1, 2]], "output": [1, 2]},
        {"input": [[5], []], "output": [5]}
      ]
    },
    "rubric": {
      "all_tests_must_pass": true
    },
    "eval": {
      "method": "python_exec",
      "params": {
        "timeout_seconds": 10
      }
    },
    "meta": {
      "difficulty": "med",
      "tags": ["algorithm", "sorting"],
      "prompt_variant_group": null
    }
  },
  {
    "id": "code-008",
    "category": "code",
    "prompt": "Write a Python function called 'is_prime' that checks if a number is prime.",
    "expected": {
      "entrypoint": "is_prime",
      "tests": [
        {"input": 2, "output": true},
        {"input": 3, "output": true},
        {"input": 4, "output": false},
        {"input": 17, "output": true},
        {"input": 1, "output": false},
        {"input": 0, "output": false}
      ]
    },
    "rubric": {
      "all_tests_must_pass": true
    },
    "eval": {
      "method": "python_exec",
      "params": {
        "timeout_seconds": 10
      }
    },
    "meta": {
      "difficulty": "med",
      "tags": ["math", "validation"],
      "prompt_variant_group": null
    }
  },
  {
    "id": "code-009",
    "category": "code",
    "prompt": "Write a Python function called 'binary_search' that takes a sorted list and a target value, returns the index if found, or -1 if not found.",
    "expected": {
      "entrypoint": "binary_search",
      "tests": [
        {"input": [[1, 2, 3, 4, 5], 3], "output": 2},
        {"input": [[1, 2, 3, 4, 5], 6], "output": -1},
        {"input": [[1], 1], "output": 0},
        {"input": [[1], 2], "output": -1}
      ]
    },
    "rubric": {
      "all_tests_must_pass": true
    },
    "eval": {
      "method": "python_exec",
      "params": {
        "timeout_seconds": 10
      }
    },
    "meta": {
      "difficulty": "hard",
      "tags": ["algorithm", "search"],
      "prompt_variant_group": null
    }
  },
  {
    "id": "code-010",
    "category": "code",
    "prompt": "Write a Python function called 'flatten' that takes a nested list and returns a flattened list.",
    "expected": {
      "entrypoint": "flatten",
      "tests": [
        {"input": [[1, 2], [3, 4]], "output": [1, 2, 3, 4]},
        {"input": [[1, [2, 3]], 4], "output": [1, 2, 3, 4]},
        {"input": [], "output": []},
        {"input": [[[[1]]]], "output": [1]}
      ]
    },
    "rubric": {
      "all_tests_must_pass": true
    },
    "eval": {
      "method": "python_exec",
      "params": {
        "timeout_seconds": 10
      }
    },
    "meta": {
      "difficulty": "hard",
      "tags": ["recursion", "nested_structures"],
      "prompt_variant_group": null
    }
  }
]
