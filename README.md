# ğŸ§ª Model Behavior Lab

A lightweight evaluation framework for analyzing the behavior of locally running LLMs using [Ollama](https://ollama.com). This project runs a set of prompts across different models and scores them on logic, emotion, hallucination, and code quality.

---

## ğŸ“Œ Features

- ğŸ” Run multiple prompts across multiple local models
- ğŸ§  Score responses using custom criteria
- ğŸ“Š Visualize results using bar plots and heatmaps
- âš¡ï¸ Uses models via `ollama` â€” fast and offline
- âœ… Modular scoring and prompt evaluation system

---

## ğŸ—‚ Project Structure

```
model-behavior-lab/
â”‚
â”œâ”€â”€ main.py                      # Runs all prompts through selected models
â”œâ”€â”€ analyze_results.py          # Converts JSON results into plots
â”œâ”€â”€ score_results.py            # (Optional) Re-scores results.json
â”‚
â”œâ”€â”€ prompts/
â”‚   â””â”€â”€ sample_set.json         # List of prompts with ids and task text
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ results.json            # Output after running all prompts
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ ollama_runner.py        # Function to query Ollama models
â”‚
â”œâ”€â”€ analyzers/
â”‚   â”œâ”€â”€ scoring.py              # Scoring logic for different task types
â”‚   â””â”€â”€ apply_scoring.py        # Routing logic to apply correct scoring
â”‚
â””â”€â”€ visualize.ipynb             # Jupyter Notebook for analysis
```

---

## ğŸ›  Installation

1. **Clone the repo**

```bash
git clone https://github.com/Chunduri-Aditya/Model-Behavior-Lab.git
cd Model-Behavior-Lab
```

2. **Set up environment**

```bash
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

3. **Install Ollama and required models**

Make sure you have [Ollama](https://ollama.com) installed and then pull the models:

```bash
ollama pull phi3:3.8b
ollama pull mistral:7b
ollama pull samantha-mistral:7b
```

---

## â–¶ï¸ Usage

### 1. Run the prompt evaluation:

```bash
python main.py
```

This will run all prompts through all models and generate `data/results.json`.

### 2. Visualize results:

You can either use:

- The Python script:

  ```bash
  python analyze_results.py
  ```

- Or the notebook:

  ```bash
  jupyter notebook visualize.ipynb
  ```

---

## ğŸ§ª Scoring Criteria

| Type         | Scoring Logic                                                                 |
|--------------|--------------------------------------------------------------------------------|
| Logic        | 1 if output includes "yes" or "no"                                             |
| Emotion      | 1 if output contains emotion-related words like "hope", "feel", etc.          |
| Hallucination| 1 if output correctly says nobody is president of Mars                        |
| Code         | 1 if output contains both `def` and `return`                                  |

Scoring logic can be customized via `analyzers/scoring.py`.

---

## ğŸ“ˆ Sample Output

(You can generate plots by running the notebook or script.)

---

## ğŸ§  Models Used

- `phi3:3.8b`
- `mistral:7b`
- `samantha-mistral:7b`

You can modify `main.py` to add or remove models.

---

## ğŸ™Œ Future Ideas

- Add GPT-4 or Claude via API (optionally)
- Extend scoring types: safety, reasoning depth, creativity
- Add prompt category tagging
- Benchmark speed vs quality

---

## ğŸ“„ License

MIT License

---

## ğŸ‘¤ Author

**Aditya Chunduri**  
ğŸ”— [github.com/Chunduri-Aditya](https://github.com/Chunduri-Aditya)  
ğŸ“ M.S. Applied Data Science, USC  
ğŸ’¡ Building AI projects with love & caffeine â˜•ï¸
